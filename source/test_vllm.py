import os
import torch
from vllm import LLM, SamplingParams
from .engine.utils import get_device_count


# 1. åŸºç¡€é…ç½®ï¼ˆé€‚é… ARM64/æ— GPUåœºæ™¯ï¼Œä¼˜å…ˆCPUæµ‹è¯•ï¼‰
model_path = "/models/qwen3-32b"  # æ¨¡å‹è·¯å¾„
gpu_available = torch.cuda.is_available()
npu_available = hasattr(torch, "npu") and torch.npu.is_available()

# 2. é‡‡æ ·å‚æ•°ï¼ˆç®€å•é…ç½®ï¼‰
sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=100,
    top_p=0.95
)

# 3. åŠ è½½æ¨¡å‹ï¼ˆå…³é”®ï¼šé€‚é…ä¸åŒè®¾å¤‡ï¼‰
print(f"å¼€å§‹åŠ è½½æ¨¡å‹ï¼š{model_path}")
print(f"CUDAå¯ç”¨ï¼š{gpu_available} | NPUå¯ç”¨ï¼š{npu_available}")

try:
    # æ ¸å¿ƒï¼šVLLM åŠ è½½æ¨¡å‹ï¼ˆCPU æ¨¡å¼éœ€æŒ‡å®š tensor_parallel_size=1 + cpu_offload=Trueï¼‰
    llm = LLM(
        model=model_path,
        tensor_parallel_size=4,  # å•å¡/CPU
        cpu_offload=False,  # æ— GPU/NPUåˆ™CPU offload
        disable_log_stats=True,  # å…³é—­ç»Ÿè®¡æ—¥å¿—ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
        trust_remote_code=True,  # è‡ªå®šä¹‰æ¨¡å‹éœ€å¼€å¯ï¼ˆå¦‚Qwenï¼‰
        dtype="auto"  # è‡ªåŠ¨é€‚é…ç²¾åº¦ï¼ˆARM64 å»ºè®® float16ï¼‰
    )
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

    # 4. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆï¼ˆéªŒè¯æ¨¡å‹å¯ç”¨ï¼‰
    prompts = ["ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"]
    outputs = llm.generate(prompts, sampling_params)

    # æ‰“å°ç”Ÿæˆç»“æœ
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"\nğŸ“ è¾“å…¥ï¼š{prompt}")
        print(f"ğŸ” è¾“å‡ºï¼š{generated_text}")

except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½/ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
    # æ‰“å°è¯¦ç»†æŠ¥é”™ï¼ˆä¾¿äºæ’æŸ¥ï¼‰
    import traceback
    traceback.print_exc()
